{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from get_tweets import get_tweets_by_search_term\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('twitter_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Tweets\n",
    "keywords = input(\"Enter keywords, hashtags separated by commas: \")\n",
    "keywords = list(set(keywords.split(\",\")))\n",
    "get_tweets_by_search_term(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tweets to pickle file and reload to DF - Avoid API calls\n",
    "this_file_path = os.path.abspath(\"__file__\")\n",
    "BASE_DIR = os.path.dirname(this_file_path)\n",
    "tweet_pickle_file = os.path.join(BASE_DIR, \"tweets\", \"tweet_list.pkl\")\n",
    "tweets_df = pd.read_pickle(tweet_pickle_file)\n",
    "tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary exploratory analysis\n",
    "print('Dataset shape:', tweets_df.shape)\n",
    "print('Dataset columns:', tweets_df.columns)\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except for Tweet and create a list of all words\n",
    "tweets_only_df = tweets_df.drop(['UserId', 'TweetId', 'location', 'created'], axis=1)\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove http links from tweets\n",
    "def remove_http_https(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "tweets_only_df['link_removed'] = tweets_only_df['tweet'].apply(lambda x: remove_http_https(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "#string.punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_punct'] = tweets_only_df['tweet'].apply(lambda x: remove_punct(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "def tokenize(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_tokenized'] = tweets_only_df['tweet_punct'].apply(lambda x: tokenize(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_nonstop'] = tweets_only_df['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_stemmed'] = tweets_only_df['tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "tweets_only_df['tweet_lemmatized'] = tweets_only_df['tweet_stemmed'].apply(lambda x: lemmatizer(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_tweets = ' '.join(word for word in tweets_only_df['tweet_punct'])\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "wordcloud_all_tweets = WordCloud(max_font_size=50, max_words=50, background_color=\"white\").generate(all_final_tweets)\n",
    "ax.imshow(wordcloud_all_tweets, interpolation='bilinear')\n",
    "ax.set_title(f'Word Cloud of Top 100 words in 5000 Tweets Mentioning {keywords} ', fontsize=20)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f9f323f011a694c5b8d7548fcbb4ff2fb8579a58e6afef7ae18f2b675d9e533"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
