{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from get_tweets import get_tweets_by_search_term, get_tweets_by_username\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('twitter_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_tweets_by_username(\"LibranTechie\", 100)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Tweets\n",
    "keywords = input(\"Enter keywords, hashtags separated by commas: \")\n",
    "keywords = list(set(keywords.split(\",\")))\n",
    "num_tweets = int(input(\"Enter number of tweets to be retrieved: \"))\n",
    "data = get_tweets_by_search_term(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tweets to pickle file and reload to DF - Avoid API calls\n",
    "this_file_path = os.path.abspath(\"__file__\")\n",
    "BASE_DIR = os.path.dirname(this_file_path)\n",
    "tweet_pickle_file = os.path.join(BASE_DIR, \"tweets\", \"tweet_list.pkl\")\n",
    "data.to_pickle(tweet_pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickle file\n",
    "tweets_df = pd.read_pickle(tweet_pickle_file)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary exploratory analysis\n",
    "print('Dataset shape:', tweets_df.shape)\n",
    "print('Dataset columns:', tweets_df.columns)\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except for Tweet and create a list of all words\n",
    "tweets_only_df = tweets_df.drop(['UserId', 'TweetId', 'location', 'created'], axis=1)\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove http links from tweets\n",
    "def remove_http_https(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "tweets_only_df['link_removed'] = tweets_only_df['tweet'].apply(lambda x: remove_http_https(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "#string.punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_punct'] = tweets_only_df['tweet'].apply(lambda x: remove_punct(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "def tokenize(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_tokenized'] = tweets_only_df['tweet_punct'].apply(lambda x: tokenize(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_nonstop'] = tweets_only_df['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_stemmed'] = tweets_only_df['tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "tweets_only_df['tweet_lemmatized'] = tweets_only_df['tweet_stemmed'].apply(lambda x: lemmatizer(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_tweets = ' '.join(word for word in tweets_only_df['tweet_punct'])\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "wordcloud_all_tweets = WordCloud(max_font_size=50, max_words=50, background_color=\"white\").generate(all_final_tweets)\n",
    "ax.imshow(wordcloud_all_tweets, interpolation='bilinear')\n",
    "ax.set_title(f'Word Cloud of Top 100 words in 5000 Tweets Mentioning {keywords} ', fontsize=20)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity of the tweets using TextBlob\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "tweets_df['subjectivity'] = tweets_df['tweet'].apply(get_subjectivity)\n",
    "tweets_df['polarity'] = tweets_df['tweet'].apply(get_polarity)\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute negative, neutral, positive analysis of the tweets\n",
    "def get_analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score== 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "tweets_df['analysis'] = tweets_df['polarity'].apply(get_analysis)\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_subjectivity(score):\n",
    "    if score < 0.5:\n",
    "        return 'Objective'\n",
    "    else:\n",
    "        return 'Subjective'\n",
    "\n",
    "tweets_df['subjectivity_analysis'] = tweets_df['subjectivity'].apply(analyse_subjectivity)\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of positive, negative and neutral tweets\n",
    "tweets_df['analysis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the polarity of the tweets\n",
    "ax0 = tweets_df['polarity'].hist(bins=20, color='blue', edgecolor='black', linewidth=1.2, figsize=(10, 6))\n",
    "ax0.set_title(f'Histogram of the Polarity of the Tweets - {keywords}', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the polarity of the tweets\n",
    "ax1 = tweets_df['analysis'].value_counts().plot(kind='bar', color=['green', 'blue', 'red'], figsize=(10, 8))\n",
    "ax1.set_title(f'Analysis of Tweets by Search Term: {keywords}', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of the polarity of the tweets\n",
    "ax3 = tweets_df['analysis'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(10, 8))\n",
    "ax3.set_title(f'% of Positive, Negative and Neutral Tweets - {keywords}', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of the subjectivity of the tweets\n",
    "ax3 = tweets_df['subjectivity_analysis'].value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(10, 8))\n",
    "ax3.set_title(f'% of Subjective Tweets - {keywords}', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polarity and subjectivity of the tweets\n",
    "ax4 = tweets_df.plot(kind='scatter', x='polarity', y='subjectivity', color='blue', figsize=(16,8))\n",
    "ax4.title.set_text(f'Sentiment Analysis of 5000 Tweets Mentioning - {keywords}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f9f323f011a694c5b8d7548fcbb4ff2fb8579a58e6afef7ae18f2b675d9e533"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
