{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from get_tweets import get_tweets_by_search_term\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/brightkoech/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/brightkoech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/brightkoech/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/brightkoech/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/brightkoech/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('twitter_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Den</td>\n",
       "      <td>1526291378887708673</td>\n",
       "      <td>@stablekwon we want the Terra and Luna together! we don't want another project ... who else can ...</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luci</td>\n",
       "      <td>1526291378832953344</td>\n",
       "      <td>@BlizzardCS I thought I would let you all know that I did find my Authenticator. Thank you all f...</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TOM$‚ú®</td>\n",
       "      <td>1526291378724130830</td>\n",
       "      <td>like I gotta be more firm with my boundaries</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vihenda</td>\n",
       "      <td>1526291378686369798</td>\n",
       "      <td>@luhya_kidd üòÇüòÇüòÇüòÇüòÇüòÇ you know it</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Josh innit</td>\n",
       "      <td>1526291378598199298</td>\n",
       "      <td>When you hear the last 30 seconds of ‚Äòfight the feeling‚Äô by MacMiller for the first team https:/...</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Leonis Tord</td>\n",
       "      <td>1526291344540442624</td>\n",
       "      <td>@TeaTheKook Why must you hurt me this way?</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Tyler üíôüè≥Ô∏è‚Äçüåàüßú‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>1526291344536154112</td>\n",
       "      <td>@1800callPaul @DilophosaurusXL This is royalty of the sea, and I will not stand for slander</td>\n",
       "      <td>Akimel O‚Äôodham/Hohokam</td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Ÿã</td>\n",
       "      <td>1526291344523673600</td>\n",
       "      <td>he's so fucking cute it pains me - j https://t.co/Mdced2IHCz</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Devon</td>\n",
       "      <td>1526291344519471110</td>\n",
       "      <td>54 doubles ü§£ü§£ü§£ his OPS during this stretch has to be well over .1000 https://t.co/EafYMsGxHG</td>\n",
       "      <td></td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Big Green Bookshop</td>\n",
       "      <td>1526291344515284992</td>\n",
       "      <td>Here's the last one today. \\n\\nUp In The Tree by Margaret Atwood. Brand new hardback, no longer ...</td>\n",
       "      <td>Hastings, England</td>\n",
       "      <td>16-May-2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  UserId              TweetId  \\\n",
       "0                    Den  1526291378887708673   \n",
       "1                   Luci  1526291378832953344   \n",
       "2                  TOM$‚ú®  1526291378724130830   \n",
       "3                Vihenda  1526291378686369798   \n",
       "4             Josh innit  1526291378598199298   \n",
       "...                  ...                  ...   \n",
       "4995         Leonis Tord  1526291344540442624   \n",
       "4996     Tyler üíôüè≥Ô∏è‚Äçüåàüßú‚Äç‚ôÇÔ∏è  1526291344536154112   \n",
       "4997                   Ÿã  1526291344523673600   \n",
       "4998               Devon  1526291344519471110   \n",
       "4999  Big Green Bookshop  1526291344515284992   \n",
       "\n",
       "                                                                                                    tweet  \\\n",
       "0     @stablekwon we want the Terra and Luna together! we don't want another project ... who else can ...   \n",
       "1     @BlizzardCS I thought I would let you all know that I did find my Authenticator. Thank you all f...   \n",
       "2                                                            like I gotta be more firm with my boundaries   \n",
       "3                                                                          @luhya_kidd üòÇüòÇüòÇüòÇüòÇüòÇ you know it   \n",
       "4     When you hear the last 30 seconds of ‚Äòfight the feeling‚Äô by MacMiller for the first team https:/...   \n",
       "...                                                                                                   ...   \n",
       "4995                                                           @TeaTheKook Why must you hurt me this way?   \n",
       "4996          @1800callPaul @DilophosaurusXL This is royalty of the sea, and I will not stand for slander   \n",
       "4997                                         he's so fucking cute it pains me - j https://t.co/Mdced2IHCz   \n",
       "4998         54 doubles ü§£ü§£ü§£ his OPS during this stretch has to be well over .1000 https://t.co/EafYMsGxHG   \n",
       "4999  Here's the last one today. \\n\\nUp In The Tree by Margaret Atwood. Brand new hardback, no longer ...   \n",
       "\n",
       "                    location      created  \n",
       "0                             16-May-2022  \n",
       "1                             16-May-2022  \n",
       "2                             16-May-2022  \n",
       "3                             16-May-2022  \n",
       "4                             16-May-2022  \n",
       "...                      ...          ...  \n",
       "4995                          16-May-2022  \n",
       "4996  Akimel O‚Äôodham/Hohokam  16-May-2022  \n",
       "4997                          16-May-2022  \n",
       "4998                          16-May-2022  \n",
       "4999       Hastings, England  16-May-2022  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the Tweets\n",
    "keywords = input(\"Enter keywords, hashtags separated by commas: \")\n",
    "keywords = list(set(keywords.split(\",\")))\n",
    "get_tweets_by_search_term(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tweets to pickle file and reload to DF - Avoid API calls\n",
    "this_file_path = os.path.abspath(\"__file__\")\n",
    "BASE_DIR = os.path.dirname(this_file_path)\n",
    "tweet_pickle_file = os.path.join(BASE_DIR, \"tweets\", \"tweet_list.pkl\")\n",
    "tweets_df = pd.read_pickle(tweet_pickle_file)\n",
    "tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary exploratory analysis\n",
    "print('Dataset shape:', tweets_df.shape)\n",
    "print('Dataset columns:', tweets_df.columns)\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except for Tweet and create a list of all words\n",
    "tweets_only_df = tweets_df.drop(['UserId', 'TweetId', 'location', 'created'], axis=1)\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove http links from tweets\n",
    "def remove_http_https(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "tweets_only_df['link_removed'] = tweets_only_df['tweet'].apply(lambda x: remove_http_https(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "#string.punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_punct'] = tweets_only_df['tweet'].apply(lambda x: remove_punct(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "def tokenize(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_tokenized'] = tweets_only_df['tweet_punct'].apply(lambda x: tokenize(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_nonstop'] = tweets_only_df['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets_only_df['tweet_stemmed'] = tweets_only_df['tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "tweets_only_df['tweet_lemmatized'] = tweets_only_df['tweet_stemmed'].apply(lambda x: lemmatizer(x))\n",
    "tweets_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_tweets = ' '.join(word for word in tweets_only_df['tweet_punct'])\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "wordcloud_all_tweets = WordCloud(max_font_size=50, max_words=50, background_color=\"white\").generate(all_final_tweets)\n",
    "ax.imshow(wordcloud_all_tweets, interpolation='bilinear')\n",
    "ax.set_title(f'Word Cloud of Top 100 words in 5000 Tweets Mentioning {keywords} ', fontsize=20)\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f9f323f011a694c5b8d7548fcbb4ff2fb8579a58e6afef7ae18f2b675d9e533"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
